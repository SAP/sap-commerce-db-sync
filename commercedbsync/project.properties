#
#  Copyright: 2022 SAP SE or an SAP affiliate company and commerce-db-synccontributors.
#  License: Apache-2.0
#
#
commercedbsync.application-context=commercedbsync-spring.xml
##
# Specifies the profile name of data source that serves as migration input
#
# @values name of the data source profile
# @optional true
##
migration.input.profiles=source
##
# Specifies the profile name of data sources that serves as migration output
#
# @values name of the data source profile
# @optional true
##
migration.output.profiles=target
##
# Specifies the driver class for the source jdbc connection
#
# @values any valid jdbc driver class
# @optional false
##
migration.ds.source.db.driver=
##
# Specifies the url for the source jdbc connection
#
# @values any valid jdbc url
# @optional false
##
migration.ds.source.db.url=
##
# Specifies the user name for the source jdbc connection
#
# @values any valid user name for the jdbc connection
# @optional false
##
migration.ds.source.db.username=
##
# Specifies the password for the source jdbc connection
#
# @values any valid password for the jdbc connection
# @optional false
##
migration.ds.source.db.password=
##
# Specifies the table prefix used on the source commerce database.
# This may be relevant if a commerce installation was initialized using 'db.tableprefix'.
#
# @values any valid commerce database table prefix.
# @optional true
##
migration.ds.source.db.tableprefix=
##
# Specifies the schema the respective commerce installation is deployed to.
#
# @values any valid schema name for the commerce installation
# @optional false
##
migration.ds.source.db.schema=
##
# Specifies the name of the type system that should be taken into account
#
# @values any valid type system name
# @optional true
##
migration.ds.source.db.typesystemname=${db.type.system.name}
##
# Specifies the suffix which is used for the source typesystem
#
# @values the suffix used for typesystem. I.e, 'attributedescriptors1' means the suffix is '1'
# @optional true
# @dependency migration.ds.source.db.typesystemname
##
migration.ds.source.db.typesystemsuffix=
##
# Specifies minimum amount of idle connections available in the source db pool
#
# @values integer value
# @optional false
##
migration.ds.source.db.connection.pool.size.idle.min=${db.pool.minIdle}
##
# Specifies maximum amount of connections in the source db pool
#
# @values integer value
# @optional false
##
migration.ds.source.db.connection.pool.size.idle.max=${db.pool.maxIdle}
##
# Specifies maximum amount of active connections in the source db pool
#
# @values integer value
# @optional false
##
migration.ds.source.db.connection.pool.size.active.max=${db.pool.maxActive}
##
# Specifies the driver class for the target jdbc connection
#
# @values any valid jdbc driver class
# @optional false
##
migration.ds.target.db.driver=${db.driver}
##
# Specifies the url for the target jdbc connection
#
# @values any valid jdbc url
# @optional false
##
migration.ds.target.db.url=${db.url}
##
# Specifies the user name for the target jdbc connection
#
# @values any valid user name for the jdbc connection
# @optional false
##
migration.ds.target.db.username=${db.username}
##
# Specifies the password for the target jdbc connection
#
# @values any valid password for the jdbc connection
# @optional false
##
migration.ds.target.db.password=${db.password}
##
# Specifies the table prefix used on the target commerce database.
# This may be relevant if a commerce installation was initialized using `${db.tableprefix}` / staged approach.
#
# @values any valid commerce database table prefix.
# @optional true
##
migration.ds.target.db.tableprefix=${db.tableprefix}
migration.ds.target.db.catalog=
##
# Specifies the schema the target commerce installation is deployed to.
#
# @values any valid schema name for the commerce installation
# @optional false
##
migration.ds.target.db.schema=dbo
##
# Specifies the name of the type system that should be taken into account
#
# @values any valid type system name
# @optional true
##
migration.ds.target.db.typesystemname=DEFAULT
##
# Specifies the suffix which is used for the target typesystem
#
# @values the suffix used for typesystem. I.e, 'attributedescriptors1' means the suffix is '1'
# @optional true
# @dependency migration.ds.source.db.typesystemname
##
migration.ds.target.db.typesystemsuffix=
##
# Specifies minimum amount of idle connections available in the target db pool
#
# @values integer value
# @optional false
##
migration.ds.target.db.connection.pool.size.idle.min=${db.pool.minIdle}
##
# Specifies maximum amount of idle connections available in the target db pool
#
# @values integer value
# @optional false
##
migration.ds.target.db.connection.pool.size.idle.max=${db.pool.maxIdle}
##
# Specifies maximum amount of connections in the target db pool
#
# @values integer value
# @optional false
##
migration.ds.target.db.connection.pool.size.active.max=${db.pool.maxActive}
##
# When using the staged approach, multiple sets of commerce tables may exists (each having its own tableprefix).
# To prevent cluttering the db, this property specifies the maximum number of table sets that can exist,
# if exceeded the schema migrator will complain and suggest a cleanup.
#
# @values integer value
# @optional true
##
migration.ds.target.db.max.stage.migrations=5
##
# Specifies whether the data migration shall be triggered by the 'update running system' operation.
#
# @values true or false
# @optional true
##
migration.trigger.updatesystem=false
##
# Globally enables / disables schema migration. If set to false, no schema changes will be applied.
#
# @values true or false
# @optional true
##
migration.schema.enabled=true
##
# Specifies if tables which are missing in the target should be added by schema migration.
#
# @values true or false
# @optional true
# @dependency migration.schema.enabled
##
migration.schema.target.tables.add.enabled=true
##
# Specifies if extra tables in target (compared to source schema) should be removed by schema migration.
#
# @values true or false
# @optional true
# @dependency migration.schema.enabled
##
migration.schema.target.tables.remove.enabled=false
##
# Specifies if columns which are missing in the target tables should be added by schema migration.
#
# @values true or false
# @optional true
# @dependency migration.schema.enabled
##
migration.schema.target.columns.add.enabled=true
##
# Specifies if extra columns in target tables (compared to source schema) should be removed by schema migration.
#
# @values true or false
# @optional true
# @dependency migration.schema.enabled
##
migration.schema.target.columns.remove.enabled=true
##
# Specifies if the schema migrator should be automatically triggered before data copy process is started
#
# @values true or false
# @optional true
# @dependency migration.schema.enabled
##
migration.schema.autotrigger.enabled=false
##
# Activate data export to external DB via cron jobs
#
# @values true or false
# @optional true
##
migration.data.export.enabled=false
# Specifies the number of rows to read per batch. This only affects tables which can be batched.
#
# @values integer value
# @optional true
##
migration.data.reader.batchsize=1000
##
# Specifies if the target tables should be truncated before data is copied over.
#
# @values true or false
# @optional true
##
migration.data.truncate.enabled=true
##
# If truncation of target tables is enabled, this property specifies tables that should be excluded from truncation.
#
# @values comma separated list of table names
# @optional true
# @dependency migration.data.truncate.enabled
##
migration.data.truncate.excluded=
##
# Specifies the number of threads used per table to write data to target.
# Note that this value applies per table, so in total the number of threads will depend on
# 'migration.data.maxparalleltablecopy'.
# [total number of writer threads] = [migration.data.workers.writer.maxtasks] * [migration.data.maxparalleltablecopy]
#
# @values integer value
# @optional true
# @dependency migration.data.maxparalleltablecopy
##
migration.data.workers.writer.maxtasks=10
##
# Specifies the number of threads used per table to read data from source.
# Note that this value applies per table, so in total the number of threads will depend on
# 'migration.data.maxparalleltablecopy'.
# [total number of reader threads] = [migration.data.workers.reader.maxtasks] * [migration.data.maxparalleltablecopy]

# @values integer value
# @optional true
# @dependency migration.data.maxparalleltablecopy
##
migration.data.workers.reader.maxtasks=3
##
# Specifies the number of retries in case a worker task fails.
#
# @values integer value
# @optional true
##
migration.data.workers.retryattempts=0
##
# Specifies the number of tables that are copied over in parallel.
#
# @values integer value
# @optional true
##
migration.data.maxparalleltablecopy=2
##
# If set to true, the migration will abort as soon as an error occured.
# If set to false, the migration will try to continue if the state of the runtime allows.
#
# @values true or false
# @optional true
##
migration.data.failonerror.enabled=true
##
# Specifies the columns to be excluded
#
# @values migration.data.columns.excluded.[tablename]=[comma separated list of column names]
# @optional true
##
migration.data.columns.excluded.attributedescriptors=
##
# Specifies the columns to be nullified. Whatever value there was will be replaced with NULL in the target column.
#
# @values migration.data.columns.nullify.[tablename]=[comma separated list of column names]
# @optional true
##
migration.data.columns.nullify.attributedescriptors=
##
# If set to true, all indices in the target table will be removed before copying over the data.
#
# @values true of false
# @optional true
##
migration.data.indices.drop.enabled=false
##
# do not recreate following indices after the migration. Comma separated values
#
# @values comma separated values
# @optional true
##
migration.data.indices.drop.recreate.exclude=
##
# If set to true, all indices in the target table will be disabled (NOT removed) before copying over the data.
# After the data copy the indices will be enabled and rebuilt again.
#
# @values true of false
# @optional true
##
migration.data.indices.disable.enabled=false
##
# If disabling of indices is enabled, this property specifies the tables that should be included.
# If no tables specified, indices for all tables will be disabled.
#
# @values comma separated list of tables
# @optional true
# @dependency migration.data.indices.disable.enabled
##
migration.data.indices.disable.included=
##
# Flag to enable the migration of audit tables.
#
# @values true or false
# @optional true
##
migration.data.tables.audit.enabled=true
##
# Specifies a list of custom tables to migrate. Custom tables are tables that are not part of the commerce type system.
#
# @values comma separated list of table names.
# @optional true
##
migration.data.tables.custom=
##
# Tables to exclude from migration (use table names name without prefix)
#
# @values comma separated list of table names.
# @optional true
##
migration.data.tables.excluded=SYSTEMINIT,StoredHttpSessions,itemdeletionmarkers,tasks_aux_queue,tasks_aux_scheduler,tasks_aux_workers
##
# Tables to include (use table names name without prefix)
#
# @values comma separated list of table names.
# @optional true
##
migration.data.tables.included=
##
# Run migration in the cluster (based on commerce cluster config). The 'HAC' node will be the primary one.
# A scheduling algorithm decides which table will run on which node. Nodes are notified using cluster events.
#
# @values true or false
# @optional true
##
migration.cluster.enabled=false
##
# If set to true, the migration will resume from where it stopped (either due to errors or cancellation).
#
# @values true or false
# @optional true
##
migration.scheduler.resume.enabled=false
##
# If set to true, the migration will run in incremental mode. Only rows that were modified after a given timestamp
# will be taken into account.
#
# @values true or false
# @optional true
##
migration.data.incremental.enabled=false
##
# Only these tables will be taken into account for incremental migration.
#
# @values comma separated list of tables.
# @optional true
# @dependency migration.data.incremental.enabled
##
migration.data.incremental.tables=
##
# Records created or modified after this timestamp will be copied only.
#
# @values The timestamp in ISO-8601 ISO_ZONED_DATE_TIME format
# @optional true
# @dependency migration.data.incremental.enabled
##
migration.data.incremental.timestamp=
##
# Specifies the timeout of the data pipe.
#
# @values integer value
# @optional true
##
migration.data.pipe.timeout=7200
##
# Specifies the capacity of the data pipe.
#
# @values integer value
# @optional true
##
migration.data.pipe.capacity=100
##
# Specifies the timeout of the migration monitor.
# If there was no activity for too long the migration will be marked as 'stalled' and aborted.
#
# @values integer value
# @optional true
##
migration.stalled.timeout=7200
##
# Specifies blob storage connection string for storing reporting files.
#
# @values any azure blob storage connection string
# @optional true
##
migration.data.report.connectionstring=${media.globalSettings.cloudAzureBlobStorageStrategy.connection}
##
# Specifies the properties that should be masked in HAC.
#
# @values any property key
# @optional true
##
migration.properties.masked=migration.data.report.connectionstring,migration.ds.source.db.password,migration.ds.target.db.password
##
# Specifies the default locale used.
#
# @values any locale
# @optional true
##
migration.locale.default=en-US
##
# Support views during data migration. String pattern for view naming convention with `'%s'` as table name. e.g. `v_%s`
#
# @values any string
# @optional true
##
migration.data.view.name.pattern=v_%s
##
# Activate DDL view generation for specific
#
# @values any string
# @optional true
##
migration.data.view.t.TABLE.enabled=false
##
# Activate DDL view generation for specific _TABLE_, with additional `JOIN` clausule
#
# @values any string
# @optional true
# @dependency migration.data.view.t.TABLE.enabled
##
migration.data.view.t.TABLE.joinWhereClause={table}
##
# Possibility to use custom functions to obfuscate values for specific columns
#
# @values any valid SQL function call
# @optional true
# @dependency migration.data.view.t.TABLE.enabled
##
migration.data.view.t.TABLE.columnTransformation.COLUMN=GETDATE()

##
# If set to true, the JDBC queries ran against the source and target data sources will be logged in the storage pointed by the property {migration.data.report.connectionstring}
#
# @values true or false
# @optional false
##
migration.log.sql=false
##
# Specifies the number of log entries to add to the in-memory collection of JDBC log entries of a JDBC queries store before flushing the collection contents into the blob file storage associated with the JDBC store's data souce and clearing the in-memory collection to free memory
#
# @values an integer number
# @optional 10,000,000
##
migration.log.sql.memory.flush.threshold.nbentries=10000000
##
# If set to true, the values of the parameters of the JDBC queries ran against the source data source will be logged in the JDBC queries logs (migration.log.sql has to be true to enable this type of logging). For security reasons, the tool will never log parameter values for the queries ran against the target datasource.
#
# @values true or false
# @optional true
##
migration.log.sql.source.showparameters=true
##
# Specifies the name of the container where the tool will store the files related to migration in the blob storage pointed by the property {migration.data.report.connectionstring}
#
# @values any string
# @optional false
##
migration.data.filestorage.container.name=migration
migration.data.fulldatabase.enabled=true
##
# Activates enhanced memory usage logging
#
# @values true or false
# @optional false
##
migration.profiling=false
##
# Delays reading until a minimum amount of memory is available
#
# @values any number
# @optional false
##
migration.memory.min=5000000
##
# Number of attempts to wait for free memory
#
# @values any number
# @optional false
##
migration.memory.attempts=300
##
# Number of time to wait for free memory (milliseconds)
#
# @values any number
# @optional false
##
migration.memory.wait=2000


# Enhanced Logging
log4j2.appender.migrationAppender.type=Console
log4j2.appender.migrationAppender.name=MigrationAppender
log4j2.appender.migrationAppender.layout.type=PatternLayout
log4j2.appender.migrationAppender.layout.pattern=%-5p [%t] [%c{1}] %X{migrationID,pipeline,clusterID} %m%n
log4j2.logger.migrationToolkit.name=com.sap.cx.boosters.commercedbsync
log4j2.logger.migrationToolkit.level=INFO
log4j2.logger.migrationToolkit.appenderRef.migration.ref=MigrationAppender
log4j2.logger.migrationToolkit.additivity=false


