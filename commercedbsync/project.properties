#
#  Copyright: 2022 SAP SE or an SAP affiliate company and commerce-db-synccontributors.
#  License: Apache-2.0
#
#
commercedbsync.application-context=commercedbsync-spring.xml

################################
# Migration specific properties
################################
migration.ds.source.db.driver=
migration.ds.source.db.url=
migration.ds.source.db.username=
migration.ds.source.db.password=
migration.ds.source.db.tableprefix=
migration.ds.source.db.schema=
migration.ds.source.db.typesystemname=DEFAULT
migration.ds.source.db.typesystemsuffix=
migration.ds.source.db.connection.removeabandoned=true
migration.ds.source.db.connection.pool.size.idle.min=${db.pool.minIdle}
migration.ds.source.db.connection.pool.size.idle.max=${db.pool.maxIdle}
migration.ds.source.db.connection.pool.size.active.max=${db.pool.maxActive}
migration.ds.target.db.driver=${db.driver}
migration.ds.target.db.url=${db.url}
migration.ds.target.db.username=${db.username}
migration.ds.target.db.password=${db.password}
migration.ds.target.db.tableprefix=${db.tableprefix}
migration.ds.target.db.catalog=
migration.ds.target.db.schema=dbo
migration.ds.target.db.typesystemname=DEFAULT
migration.ds.target.db.typesystemsuffix=
migration.ds.target.db.connection.removeabandoned=true
migration.ds.target.db.connection.pool.size.idle.min=${db.pool.minIdle}
migration.ds.target.db.connection.pool.size.idle.max=${db.pool.maxIdle}
migration.ds.target.db.connection.pool.size.active.max=${db.pool.maxActive}
migration.ds.target.db.max.stage.migrations=5
#triggered by updatesystem process or manually by hac
migration.trigger.updatesystem=false
# Schema migration section - parameters for copying schema from source to target
migration.schema.enabled=true
migration.schema.target.tables.add.enabled=true
migration.schema.target.tables.remove.enabled=false
migration.schema.target.columns.add.enabled=true
migration.schema.target.columns.remove.enabled=true
# automatically trigger schema migrator before data copy process is started
migration.schema.autotrigger.enabled=false
# the number of rows read per iteration
migration.data.reader.batchsize=1000
# delete rows in target table before inserting new records
migration.data.truncate.enabled=true
# These tables will not be emptied before records are inserted
migration.data.truncate.excluded=
# maximum number of writer workers per table that can be executed in parallel within a single node in the cluster
migration.data.workers.writer.maxtasks=10
# maximum number of reader workers per table that can be executed in parallel within a single node in the cluster
migration.data.workers.reader.maxtasks=3
# max retry attempts of a worker in case there is a problem
migration.data.workers.retryattempts=0
# maximum number of table that can be copied in parallel within a single node in the cluster
migration.data.maxparalleltablecopy=2
# ignores data insertion errors and continues to the next records
migration.data.failonerror.enabled=true
# columns to be excluded. format: migration.data.columns.excluded.<tablename>=<list of column names>
migration.data.columns.excluded.attributedescriptors=
migration.data.columns.nullify.attributedescriptors=
#remove all indices
migration.data.indices.drop.enabled=false
#do not recreate following indices after the migration. Comma separated values
migration.data.indices.drop.recreate.exclude=
#disable indices during migration
migration.data.indices.disable.enabled=false
#if empty, disable indices on all tables. If table specified, only disable for this one.
migration.data.indices.disable.included=
#flag to enable the migration of audit tables
migration.data.tables.audit.enabled=true
#custom tables to migrate (use comma-separated list)
migration.data.tables.custom=
#tables to exclude (use table names name without prefix)
migration.data.tables.excluded=SYSTEMINIT,StoredHttpSessions
#tables to include (use table names name without prefix)
migration.data.tables.included=
migration.cluster.enabled=false
#enable the incremental database migration.
migration.data.incremental.enabled=false
#Only these tables will be taken into account for incremental migration.
migration.data.incremental.tables=
#The timestamp in ISO-8601 ISO_ZONED_DATE_TIME format. Records created or modified after this timestamp will be copied only.
migration.data.incremental.timestamp=
#EXPERIMENTAL: Enable bulk copy for better performance
migration.data.bulkcopy.enabled=false
migration.data.pipe.timeout=7200
migration.data.pipe.capacity=100
# No activity? -> migration aborted and marked as stalled
migration.stalled.timeout=7200
migration.data.timeout=60
migration.data.report.connectionstring=${media.globalSettings.cloudAzureBlobStorageStrategy.connection}
# Properties that will be masked in the report
migration.properties.masked=migration.data.report.connectionstring,migration.ds.source.db.password,migration.ds.target.db.password
migration.locale.default=en-US
# support views during data migration
## string pattern for view naming convention with '%s' as table name. e.g. v_%s
migration.data.view.name.pattern=v_%s 
# DDL View Generation
# more information on https://github.tools.sap/cx-boosters/sap-commerce-db-sync/wiki/Dynamic-View-Generation
migration.data.view.t.{table}.enabled=false
migration.data.view.t.{table}.joinWhereClause={table}
migration.data.view.t.{table}.columnTransformation.{column}=GETDATE()


